{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this folder, merged_file.csv is the data set from all NHTSA files, and NHTSA_ADS_cleaned.csv is the cleaned data from ../california_data/NHTSA _SGO_incident_reports/SGO-2021-01_Incident_Reports_ADS.csv\n",
    "1. delete if \"NHTSA_ADS_cleaned.csv\" is existed\n",
    "2. read ../california_data/NHTSA _SGO_incident_reports/SGO-2021-01_Incident_Reports_ADS.csv\n",
    "\n",
    "## Steps to processing data (columns)\n",
    "1. Report ID -- no change\n",
    "2. Report Version -- no change\n",
    "3. Reporting Entity -- no change\n",
    "4. Report Type -- one hot encoded\n",
    "5. Report Month -- no change\n",
    "6. Report Year -- no change\n",
    "7. Report Submission Date -- break down to year and month\n",
    "8. VIN -- no change\n",
    "9. VIN - Unknown -- one hot encoded\n",
    "10. Serial Number -- dropped, no information provided\n",
    "11. Make, model -- no change\n",
    "12. Model -- Unknown - there's only 1 'Y' in this column, so I dropped the row with 'Y', and dropped 'Model - Unknown' col\n",
    "13. Model Year -- no change\n",
    "14. Model Year -- Unknown - there's only 4 'Y', so same as above\n",
    "15. Same Vehicle ID, Mileage -- no change\n",
    "16. Mileage - Unknown -- one hot encoded\n",
    "17. Driver / Operator Type -- one hot encoded, with \"None\", \"Other, see Narrative\" and \"Unknown\" combined into the same category, also put 1 on both \"In-Vehicle\" col and \"Remote\" col instead of have a category for \"In-Vehicle and Remote (Commercial / Test)\"\n",
    "18. ADAS/ADS System Version, ADAS/ADS Hardware Version, ADAS/ADS Software Version -- no change\n",
    "19. ADAS/ADS System Version - Unk, ADAS/ADS System Version CBI, ADAS/ADS Hardware Version - Unk, ADAS/ADS Hardware Version CBI, ADAS/ADS Software Version - Unk, ADAS/ADS Software Version CBI -- one hot encoded\n",
    "20. Other Reporting Entities? -- no change\n",
    "21. Other Reporting Entities? -- Unk, Other Reporting Entities? - NA - one hot encoded\n",
    "22. Federal Regulatory Exemption? -- no change(less information provided, seems not important?)\n",
    "23. Other Federal Reg. Exemption -- dropped, no info\n",
    "24. Federal Reg. Exemption - Unk, Federal Reg. Exemption - No, State or Local Permit? -- one hot encoded, for 'State or Local Permit?' I put 0 for unknown as well\n",
    "25. State or Local Permit -- no change\n",
    "26. ADS Equipped?, Automation System Engaged? -- one hot encoded\n",
    "27. Operating Entity -- no change\n",
    "28. Operating Entity - Unknown -- dropped, no info\n",
    "29. Source - Complaint/Claim,Source - Telematics,Source - Law Enforcement,Source - Field Report,Source - Testing,Source - Media,Source - Other -- one hot encoded\n",
    "30. Source - Other Text -- no change\n",
    "31. Incident Date, Notice Received Date -- break down into year and month\n",
    "32. Incident Date - Unknown, Notice Received Date - Unknown -- one hot encoded\n",
    "33. Incident Time (24:00), Same Incident ID -- no change\n",
    "34. Latitude, Latitude - Unknown, Longitude, Longitude - Unknown, Address, Address - Unknown, Zip Code, Zip Code - Unknown -- dropped, no info\n",
    "35. City, State -- no change\n",
    "36. City - Unknown -- one hot encoded\n",
    "37. Roadway Type, Roadway Surface, Roadway Description -- one hot encoded\n",
    "38. Posted Speed Limit -- no change\n",
    "39. Posted Speed Limit -- Unknown - one hot encoded\n",
    "40. Lighting, Weather - Clear, Weather - Snow, Weather - Cloudy, Weather - Fog/Smoke, Weather - Rain, Weather - Severe Wind, Weather - Unknown, Weather - Other -- one hot encoded ('Weather - Unknown' and 'Weather - other' are cols that provided on the original data, maybe we should combine them into the same category?)\n",
    "41. Weather - Other Text -- no change\n",
    "42. Crash With -- no change, this col indicate what type of car did the vehicle crash with, seems reasonable ot encoded this?\n",
    "43. Highest Injury Severity Alleged, Property Damage? -- one hot encoded\n",
    "44. CP Pre-Crash Movement -- no change\n",
    "45. CP Any Air Bags Deployed?, CP Was Vehicle Towed?, CP Contact Area - Rear Left, CP Contact Area - Left, CP Contact Area - Front Left, CP Contact Area - Rear, CP Contact Area - Top, CP Contact Area - Front, CP Contact Area - Rear Right, CP Contact Area - Right, CP Contact Area - Front Right, CP Contact Area - Bottom, CP Contact Area - Unknown -- one hot encoded\n",
    "46. SV Pre-Crash Movement -- no change\n",
    "47. SV Any Air Bags Deployed?, SV Was Vehicle Towed?, SV Were All Passengers Belted? -- one hot encoded\n",
    "48. SV Precrash Speed (MPH) -- no change\n",
    "49. SV Pre-crash Speed - Unknown, SV Contact Area - Rear Left, SV Contact Area - Left, SV Contact Area - Front Left, SV Contact Area - Rear, SV Contact Area - Top, SV Contact Area - Front, SV Contact Area - Rear Right, SV Contact Area - Right, SV Contact Area - Front Right, SV Contact Area - Bottom, SV Contact Area - Unknown, Data Availability - EDR, Data Availability - Police Rpt, Data Availability - Telematics, Data Availability - Complaints, Data Availability - Video, Data Availability - Other, Data Availability - Unknown -- one hot encoded\n",
    "50. Data Availability - No Data -- dropped, no info\n",
    "\n",
    "*for all one hot encoded cols after 'CP Any Air Bags Deployed?', 1 = Yes or 'Y', 0 = 'N', 'No','Unknown', or 'Not Applicable'\n",
    "*original cols are dropped after one hot encoded if the col contains multiple types\n",
    "\n",
    "cols after 'Law Enforcement Investigating?' have not been processed!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1577,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot_encoded = pd.get_dummies(merged_df['Report Type'], prefix='Report_Type')\n",
    "\n",
    "desired_order = [\n",
    "    'Report_Type_1 Day Update', \n",
    "    'Report_Type_5 Day Update', \n",
    "    'Report_Type_10 Day Update', \n",
    "    'Report_Type_Monthly Report', \n",
    "    'Report_Type_Updated, but no time indicated', \n",
    "]\n",
    "\n",
    "one_hot_encoded = one_hot_encoded.reindex(columns=desired_order)\n",
    "\n",
    "for col in reversed(one_hot_encoded.columns):\n",
    "    merged_df.insert(merged_df.columns.get_loc('Report Type') + 1, col, one_hot_encoded[col])\n",
    "\n",
    "merged_df.drop(columns= 'Report Type', inplace= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1578,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NHTSA_ADS_cleaned.csv has been deleted.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "\n",
    "file_path = \"NHTSA_ADS_cleaned.csv\"\n",
    "if os.path.exists(file_path):\n",
    "    os.remove(file_path)\n",
    "    print(f\"{file_path} has been deleted.\")\n",
    "else:\n",
    "    print(f\"{file_path} does not exist.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge all NHTSA file into one csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1579,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file_names = [\"../california_data/NHTSA _SGO_incident_reports/SGO-2021-01_Incident_Reports_ADAS_March2024.csv\", \n",
    "#               \"../california_data/NHTSA _SGO_incident_reports/SGO-2021-01_Incident_Reports_ADAS.csv\", \n",
    "#               \"../california_data/NHTSA _SGO_incident_reports/SGO-2021-01_Incident_Reports_ADS.csv\", \n",
    "#               \"../california_data/NHTSA _SGO_incident_reports/SGO-2021-01_Incident_Reports_March2024.csv\", \n",
    "#               \"../california_data/NHTSA _SGO_incident_reports/SGO-2021-01_Incident_Reports_OTHER.csv\"]\n",
    "\n",
    "# dataframes = []\n",
    "# for file in file_names:\n",
    "#     df = pd.read_csv(file)\n",
    "#     dataframes.append(df)\n",
    "\n",
    "# merged_df = pd.concat(dataframes, ignore_index=True)\n",
    "# merged_df.to_csv(\"merged_file.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1580,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_names = [\"../california_data/NHTSA _SGO_incident_reports/SGO-2021-01_Incident_Reports_ADS.csv\"]\n",
    "\n",
    "dataframes = []\n",
    "for file in file_names:\n",
    "    df = pd.read_csv(file)\n",
    "    dataframes.append(df)\n",
    "\n",
    "merged_df = pd.concat(dataframes, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing columns of merged data\n",
    "check entity count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1581,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reporting Entity count:  34\n",
      "Waymo LLC                                   196\n",
      "Cruise LLC                                  126\n",
      "General Motors, LLC                         119\n",
      "Transdev Alternative Services                97\n",
      "Zoox, Inc.                                   39\n",
      "Argo AI                                      20\n",
      "Ford Motor Company                           17\n",
      "May Mobility                                 13\n",
      "Toyota Motor Engineering & Manufacturing      7\n",
      "Pony.ai                                       6\n",
      "Mercedes-Benz USA, LLC                        6\n",
      "Easymile Inc.                                 6\n",
      "WeRide Corp                                   6\n",
      "Kodiak Robotics                               4\n",
      "Lucid USA, Inc.                               4\n",
      "Motional                                      4\n",
      "Beep, Inc.                                    4\n",
      "PACCAR Incorporated                           3\n",
      "Navistar, Inc.                                3\n",
      "Aurora Operations, Inc.                       3\n",
      "Hyundai Motor America                         3\n",
      "Robotic Research                              2\n",
      "Daimler Trucks North America, LLC             2\n",
      "TORC Robotics, Inc.                           2\n",
      "Apple Inc.                                    2\n",
      "TuSimple                                      1\n",
      "NAVYA Inc.                                    1\n",
      "Robert Bosch, LLC                             1\n",
      "Chrysler (FCA US, LLC)                        1\n",
      "First Transit                                 1\n",
      "Volvo Car USA, LLC                            1\n",
      "Ghost Autonomy Inc.                           1\n",
      "NVIDIA CORP                                   1\n",
      "Local Motors Industries                       1\n",
      "Name: Reporting Entity, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "unique_entities = merged_df['Reporting Entity'].nunique()\n",
    "print(\"Reporting Entity count: \", unique_entities)\n",
    "# pd.set_option('display.max_rows', None)\n",
    "print(merged_df['Reporting Entity'].value_counts())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is hard and it seems like meaningless to encode the entities' name\n",
    "<br>if you want to drop the \"Reporting Entity\" col, use: merged_df.drop(columns= 'Reporting Entity', inplace= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1582,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Monthly Report                    408\n",
      "10 Day Update                     144\n",
      "5 Day Update                       58\n",
      "Updated, but no time indicated     57\n",
      "1 Day Update                       36\n",
      "Name: Report Type, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "merged_df['Report Type'] = merged_df['Report Type'].replace({\n",
    "    \"1-Day\": \"1 Day Update\",\n",
    "    \"5-Day\": \"5 Day Update\",\n",
    "    \"10-Day Update\": \"10 Day Update\",\n",
    "    \"Update\": \"Updated, but no time indicated\",\n",
    "    \"Monthly\": \"Monthly Report\",\n",
    "    \"No New or Updated Incident Reports\": \"Updated, but no time indicated\"\n",
    "})\n",
    "print(merged_df['Report Type'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using One-hot to encode report type, then drop \"report type col\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1583,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot_encoded = pd.get_dummies(merged_df['Report Type'], prefix='Report_Type')\n",
    "\n",
    "desired_order = [\n",
    "    'Report_Type_1 Day Update', \n",
    "    'Report_Type_5 Day Update', \n",
    "    'Report_Type_10 Day Update', \n",
    "    'Report_Type_Monthly Report', \n",
    "    'Report_Type_Updated, but no time indicated', \n",
    "]\n",
    "\n",
    "one_hot_encoded = one_hot_encoded.reindex(columns=desired_order)\n",
    "\n",
    "for col in reversed(one_hot_encoded.columns):\n",
    "    merged_df.insert(merged_df.columns.get_loc('Report Type') + 1, col, one_hot_encoded[col])\n",
    "\n",
    "merged_df.drop(columns= 'Report Type', inplace= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1584,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "430\n",
      "430\n"
     ]
    }
   ],
   "source": [
    "print(merged_df['Report Year'].value_counts().sum())\n",
    "print(merged_df['Report Month'].value_counts().sum())\n",
    "# merged_df.drop(columns= 'Report Year', inplace= True)\n",
    "# merged_df.drop(columns= 'Report Month', inplace= True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Break submission date into year and month, drop the original col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1585,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df['Report Submission Date'] = pd.to_datetime(merged_df['Report Submission Date'], format='%b-%Y', errors='coerce')\n",
    "merged_df.insert(merged_df.columns.get_loc('Report Submission Date') + 1, 'Submission Year', merged_df['Report Submission Date'].dt.year)\n",
    "merged_df.insert(merged_df.columns.get_loc('Report Submission Date') + 2, 'Submission Month', merged_df['Report Submission Date'].dt.month)\n",
    "merged_df.drop(columns= 'Report Submission Date', inplace= True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1586,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cols below seems less important\n",
    "# merged_df.drop(columns= 'VIN', inplace= True)\n",
    "# merged_df.drop(columns= 'VIN - Unknown', inplace= True)\n",
    "\n",
    "merged_df['VIN - Unknown'] = merged_df['VIN - Unknown'].apply(lambda x: 1 if x == 'Y' else 0)\n",
    "\n",
    "merged_df.drop(columns= 'Serial Number', inplace= True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not sure what should I do for 'make' col and 'Model' col, one-hot encoding seems like reasonable?\n",
    "<br>\n",
    "Drop row if 'Model - Unknown' col is 'Y', then drop the 'Model - Unknown' col\n",
    "<br>\n",
    "do the same thing for 'Model Year - Unknown' col and 'milage' col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1587,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31\n",
      "55\n"
     ]
    }
   ],
   "source": [
    "print(merged_df['Make'].nunique())\n",
    "print(merged_df['Model'].nunique())\n",
    "merged_df = merged_df[merged_df['Model - Unknown'] != 'Y']\n",
    "merged_df.drop(columns= 'Model - Unknown', inplace= True)\n",
    "merged_df = merged_df[merged_df['Model Year - Unknown'] != 'Y']\n",
    "merged_df.drop(columns= 'Model Year - Unknown', inplace= True)\n",
    "merged_df['Mileage - Unknown'] = merged_df['Mileage - Unknown'].apply(lambda x: 1 if x == 'Y' else 0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "using one-hot to encode this col, (combine other and none, if the operator type is 'In-Vehicle and Remote', put 1 on both 'In-Vehicle' and 'Remote', instead of have a unique col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1588,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df['Driver / Operator Type'] = merged_df['Driver / Operator Type'].replace({\n",
    "    \"In-Vehicle (Commercial / Test)\": \"In-Vehicle\",\n",
    "    \"Remote (Commercial / Test)\": \"Remote\",\n",
    "    \"None\": \"None or Other\",\n",
    "    \"Other, see Narrative\": \"None or Other\",\n",
    "    \"Unknown\": \"None or Other\"\n",
    "})\n",
    "# print(merged_df['Driver / Operator Type'].value_counts())\n",
    "\n",
    "one_hot_encoded = pd.get_dummies(merged_df['Driver / Operator Type'], prefix='Driver / Operator Type')\n",
    "\n",
    "if 'Driver / Operator Type_In-Vehicle and Remote (Commercial / Test)' in one_hot_encoded.columns:\n",
    "    one_hot_encoded['Driver / Operator Type_In-Vehicle'] |= one_hot_encoded['Driver / Operator Type_In-Vehicle and Remote (Commercial / Test)']\n",
    "    one_hot_encoded['Driver / Operator Type_Remote'] |= one_hot_encoded['Driver / Operator Type_In-Vehicle and Remote (Commercial / Test)']\n",
    "    one_hot_encoded = one_hot_encoded.drop(columns=['Driver / Operator Type_In-Vehicle and Remote (Commercial / Test)'])\n",
    "\n",
    "desired_order = [\n",
    "    'Driver / Operator Type_Consumer', \n",
    "    'Driver / Operator Type_In-Vehicle', \n",
    "    'Driver / Operator Type_Remote', \n",
    "    'Driver / Operator Type_None or Other', \n",
    "]\n",
    "\n",
    "one_hot_encoded = one_hot_encoded.reindex(columns=desired_order)\n",
    "\n",
    "for col in reversed(one_hot_encoded.columns):\n",
    "    merged_df.insert(merged_df.columns.get_loc('Driver / Operator Type') + 1, col, one_hot_encoded[col])\n",
    "\n",
    "merged_df.drop(columns= 'Driver / Operator Type', inplace= True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "process all data with UNK and CBI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1589,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5th Generation ADS                                           211\n",
      "[REDACTED, MAY CONTAIN CONFIDENTIAL BUSINESS INFORMATION]    192\n",
      "5th Generation                                                30\n",
      "4                                                             20\n",
      "4th Generation ADS                                            15\n",
      "L4                                                             6\n",
      "Voyager 11 (21.02.4)                                           3\n",
      "Level 2                                                        2\n",
      "DRIVE PILOT                                                    2\n",
      "Level 4                                                        2\n",
      "Test Version 04062022                                          2\n",
      "53.22.10.03                                                    1\n",
      "Pilot Assist                                                   1\n",
      "95.22.04.22                                                    1\n",
      "36.22.30.08                                                    1\n",
      "04.22.09.01                                                    1\n",
      "80.22.10.14                                                    1\n",
      "86.22.08.25                                                    1\n",
      "105.2022.06.28                                                 1\n",
      "57.22.29.03                                                    1\n",
      "Bendix Lane Keep assist                                        1\n",
      "20.22.11.15                                                    1\n",
      "62.22.03.22                                                    1\n",
      "30.22.02.05                                                    1\n",
      "10.22.01.11                                                    1\n",
      "23.21.12.09                                                    1\n",
      "Waymo LLC                                                      1\n",
      "45.21.10.21                                                    1\n",
      "14.21.09.21                                                    1\n",
      "4.11.3                                                         1\n",
      "Olli 1.0                                                       1\n",
      "Robotic Research Autodrive                                     1\n",
      "28.21.09.02                                                    1\n",
      "21.21.08.10                                                    1\n",
      "20.21.07.24                                                    1\n",
      "98.22.11.14                                                    1\n",
      "66.23.02.05                                                    1\n",
      "111.22.11.20                                                   1\n",
      "2022.28.11                                                     1\n",
      "386c67303c1d6326bb95d4b81bc370978cd9d007                       1\n",
      "Bendix Lane Keep Assist - Version unknown                      1\n",
      "Cruise ADS                                                     1\n",
      "966cc9e370a341f1212d389d74e6f1eb5da9bdfe                       1\n",
      "20230319_RC117                                                 1\n",
      "90.23.01.22                                                    1\n",
      "144.23.05.17                                                   1\n",
      "86.23.05.15                                                    1\n",
      "6d38a25f0acc8e31bdfcf3a40215ecae4674aefc                       1\n",
      "Highway Drive Assist (L2)                                      1\n",
      "37.22.06.01                                                    1\n",
      "98.22.06.11                                                    1\n",
      "74.22.02.06                                                    1\n",
      "51.22.05.20                                                    1\n",
      "10.22.22.07                                                    1\n",
      "30.22.23.08                                                    1\n",
      "18.23.02.07                                                    1\n",
      "96.23.02.08                                                    1\n",
      "96.23.02.11                                                    1\n",
      "126.23.04.11                                                   1\n",
      "69.23.03.20                                                    1\n",
      "112.23.03.17                                                   1\n",
      "110.23.1.1                                                     1\n",
      "77106                                                          1\n",
      "6.1.4                                                          1\n",
      "22.21.07.27                                                    1\n",
      "Name: ADAS/ADS System Version, dtype: int64\n",
      "12\n",
      "12\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "print(merged_df['ADAS/ADS System Version'].value_counts())\n",
    "# print(merged_df['ADAS/ADS System Version CBI'].value_counts())\n",
    "merged_df['ADAS/ADS System Version - Unk'] = merged_df['ADAS/ADS System Version - Unk'].apply(lambda x: 1 if x == 'Y' else 0)\n",
    "merged_df['ADAS/ADS System Version CBI'] = merged_df['ADAS/ADS System Version CBI'].apply(lambda x: 1 if x == 'Y' else 0)\n",
    "print(merged_df['ADAS/ADS Hardware Version'].nunique())\n",
    "merged_df['ADAS/ADS Hardware Version - Unk'] = merged_df['ADAS/ADS Hardware Version - Unk'].apply(lambda x: 1 if x == 'Y' else 0)\n",
    "merged_df['ADAS/ADS Hardware Version CBI'] = merged_df['ADAS/ADS Hardware Version CBI'].apply(lambda x: 1 if x == 'Y' else 0)\n",
    "print(merged_df['ADAS/ADS Software Version'].nunique())\n",
    "merged_df['ADAS/ADS Software Version - Unk'] = merged_df['ADAS/ADS Software Version - Unk'].apply(lambda x: 1 if x == 'Y' else 0)\n",
    "merged_df['ADAS/ADS Software Version CBI'] = merged_df['ADAS/ADS Software Version CBI'].apply(lambda x: 1 if x == 'Y' else 0)\n",
    "print(merged_df['Other Reporting Entities?'].nunique())\n",
    "merged_df['Other Reporting Entities? - Unk'] = merged_df['Other Reporting Entities? - Unk'].apply(lambda x: 1 if x == 'Y' else 0)\n",
    "merged_df['Other Reporting Entities? - NA'] = merged_df['Other Reporting Entities? - NA'].apply(lambda x: 1 if x == 'Y' else 0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dropping 'Other Federal Reg. Exemption' col since it's completely blank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1590,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.drop(columns= 'Other Federal Reg. Exemption', inplace= True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "process data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1591,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df['Federal Reg. Exemption - Unk'] = merged_df['Federal Reg. Exemption - Unk'].apply(lambda x: 1 if x == 'Y' else 0)\n",
    "merged_df['Federal Reg. Exemption - No'] = merged_df['Federal Reg. Exemption - No'].apply(lambda x: 1 if x == 'Y' else 0)\n",
    "merged_df['State or Local Permit?'] = merged_df['State or Local Permit?'].apply(lambda x: 1 if x == 'Yes (specify below)' else 0)\n",
    "merged_df['ADS Equipped?'] = merged_df['ADS Equipped?'].apply(lambda x: 1 if x == 'Yes' else 0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One-Hot on 'Automation System Engaged?' col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1592,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot_encoded = pd.get_dummies(merged_df['Automation System Engaged?'], prefix='Automation System Engaged')\n",
    "\n",
    "# Insert the one-hot encoded columns after the original 'Automation System Engaged?' column\n",
    "for col in reversed(one_hot_encoded.columns):\n",
    "    merged_df.insert(merged_df.columns.get_loc('Automation System Engaged?') + 1, col, one_hot_encoded[col])\n",
    "\n",
    "merged_df.drop(columns= 'Automation System Engaged?', inplace= True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "processing data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1593,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.drop(columns= 'Operating Entity - Unknown', inplace= True)\n",
    "merged_df['Source - Complaint/Claim'] = merged_df['Source - Complaint/Claim'].apply(lambda x: 1 if x == 'Y' else 0)\n",
    "merged_df['Source - Telematics'] = merged_df['Source - Telematics'].apply(lambda x: 1 if x == 'Y' else 0)\n",
    "merged_df['Source - Law Enforcement'] = merged_df['Source - Law Enforcement'].apply(lambda x: 1 if x == 'Y' else 0)\n",
    "merged_df['Source - Field Report'] = merged_df['Source - Field Report'].apply(lambda x: 1 if x == 'Y' else 0)\n",
    "merged_df['Source - Testing'] = merged_df['Source - Testing'].apply(lambda x: 1 if x == 'Y' else 0)\n",
    "merged_df['Source - Media'] = merged_df['Source - Media'].apply(lambda x: 1 if x == 'Y' else 0)\n",
    "merged_df['Source - Other'] = merged_df['Source - Other'].apply(lambda x: 1 if x == 'Y' else 0)\n",
    "merged_df['Incident Date - Unknown'] = merged_df['Incident Date - Unknown'].apply(lambda x: 1 if x == 'Y' else 0)\n",
    "merged_df['Incident Time - Unknown'] = merged_df['Incident Time - Unknown'].apply(lambda x: 1 if x == 'Y' else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1594,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df['Incident Date'] = pd.to_datetime(merged_df['Incident Date'], format='%b-%Y', errors='coerce')\n",
    "merged_df.insert(merged_df.columns.get_loc('Incident Date') + 1, 'Incident Year', merged_df['Incident Date'].dt.year)\n",
    "merged_df.insert(merged_df.columns.get_loc('Incident Date') + 2, 'Incident Month', merged_df['Incident Date'].dt.month)\n",
    "merged_df.drop(columns= 'Incident Date', inplace= True)\n",
    "\n",
    "merged_df['Notice Received Date'] = pd.to_datetime(merged_df['Notice Received Date'], format='%b-%Y', errors='coerce')\n",
    "merged_df.insert(merged_df.columns.get_loc('Notice Received Date') + 1, 'Notice Received Year', merged_df['Notice Received Date'].dt.year)\n",
    "merged_df.insert(merged_df.columns.get_loc('Notice Received Date') + 2, 'Notice Received Month', merged_df['Notice Received Date'].dt.month)\n",
    "merged_df.drop(columns= 'Notice Received Date', inplace= True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dropping latitude, longitude and address, zip code, because they are \"[MAY CONTAIN PERSONALLY IDENTIFIABLE INFORMATION]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1595,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.drop(columns= 'Latitude', inplace= True)\n",
    "merged_df.drop(columns= 'Latitude - Unknown', inplace= True)\n",
    "merged_df.drop(columns= 'Longitude', inplace= True)\n",
    "merged_df.drop(columns= 'Longitude - Unknown', inplace= True)\n",
    "merged_df.drop(columns= 'Address', inplace= True)\n",
    "merged_df.drop(columns= 'Address - Unknown', inplace= True)\n",
    "merged_df.drop(columns= 'Zip Code', inplace= True)\n",
    "merged_df.drop(columns= 'Zip Code - Unknown', inplace= True)\n",
    "\n",
    "merged_df['City - Unknown'] = merged_df['City - Unknown'].apply(lambda x: 1 if x == 'Y' else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1596,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot_encoded = pd.get_dummies(merged_df['Roadway Type'], prefix='Roadway Type')\n",
    "\n",
    "# put 'unknown' col to the end\n",
    "columns = [col for col in one_hot_encoded.columns if 'Unknown' not in col]\n",
    "columns.append([col for col in one_hot_encoded.columns if 'Unknown' in col][0])\n",
    "one_hot_encoded = one_hot_encoded[columns]\n",
    "\n",
    "for col in reversed(one_hot_encoded.columns):\n",
    "    merged_df.insert(merged_df.columns.get_loc('Roadway Type') + 1, col, one_hot_encoded[col])\n",
    "\n",
    "merged_df.drop(columns= 'Roadway Type', inplace= True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1597,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df['Roadway Surface'] = merged_df['Roadway Surface'].replace({\n",
    "    'Other, see Narrative' : 'Other'\n",
    "})\n",
    "\n",
    "one_hot_encoded = pd.get_dummies(merged_df['Roadway Surface'], prefix='Roadway Surface')\n",
    "\n",
    "for col in reversed(one_hot_encoded.columns):\n",
    "    merged_df.insert(merged_df.columns.get_loc('Roadway Surface') + 1, col, one_hot_encoded[col])\n",
    "\n",
    "merged_df.drop(columns= 'Roadway Surface', inplace= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1598,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df['Roadway Description'] = merged_df['Roadway Description'].replace({\n",
    "    'Other, see Narrative' : 'Other, Unknown or Missing',\n",
    "    'Missing / Degraded Markings' : 'Other, Unknown or Missing',\n",
    "    'Unknown' : 'Other, Unknown or Missing'\n",
    "\n",
    "})\n",
    "\n",
    "one_hot_encoded = pd.get_dummies(merged_df['Roadway Description'], prefix='Roadway Description')\n",
    "\n",
    "# put 'Other, Unknown or Missing' col to the end\n",
    "columns = [col for col in one_hot_encoded.columns if 'Other, Unknown or Missing' not in col]\n",
    "columns.append([col for col in one_hot_encoded.columns if 'Other, Unknown or Missing' in col][0])\n",
    "one_hot_encoded = one_hot_encoded[columns]\n",
    "\n",
    "for col in reversed(one_hot_encoded.columns):\n",
    "    merged_df.insert(merged_df.columns.get_loc('Roadway Description') + 1, col, one_hot_encoded[col])\n",
    "\n",
    "merged_df.drop(columns= 'Roadway Description', inplace= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1599,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df['Posted Speed Limit - Unknown'] = merged_df['Posted Speed Limit - Unknown'].apply(lambda x: 1 if x == 'Y' else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1600,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df['Lighting'] = merged_df['Lighting'].replace({\n",
    "    'Other, see Narrative' : 'Other or Unknown',\n",
    "    'Unknown' : 'Other or Unknown'\n",
    "})\n",
    "\n",
    "one_hot_encoded = pd.get_dummies(merged_df['Lighting'], prefix='Lighting')\n",
    "\n",
    "# put 'unknown' col to the end\n",
    "columns = [col for col in one_hot_encoded.columns if 'Lighting_Dark - Unknown Lighting' not in col]\n",
    "columns.append([col for col in one_hot_encoded.columns if 'Lighting_Dark - Unknown Lighting' in col][0])\n",
    "one_hot_encoded = one_hot_encoded[columns]\n",
    "\n",
    "for col in reversed(one_hot_encoded.columns):\n",
    "    merged_df.insert(merged_df.columns.get_loc('Lighting') + 1, col, one_hot_encoded[col])\n",
    "\n",
    "merged_df.drop(columns= 'Lighting', inplace= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1601,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df['Weather - Clear'] = merged_df['Weather - Clear'].apply(lambda x: 1 if x == 'Y' else 0)\n",
    "merged_df['Weather - Snow'] = merged_df['Weather - Snow'].apply(lambda x: 1 if x == 'Y' else 0)\n",
    "merged_df['Weather - Cloudy'] = merged_df['Weather - Cloudy'].apply(lambda x: 1 if x == 'Y' else 0)\n",
    "merged_df['Weather - Fog/Smoke'] = merged_df['Weather - Fog/Smoke'].apply(lambda x: 1 if x == 'Y' else 0)\n",
    "merged_df['Weather - Rain'] = merged_df['Weather - Rain'].apply(lambda x: 1 if x == 'Y' else 0)\n",
    "merged_df['Weather - Severe Wind'] = merged_df['Weather - Severe Wind'].apply(lambda x: 1 if x == 'Y' else 0)\n",
    "merged_df['Weather - Unknown'] = merged_df['Weather - Unknown'].apply(lambda x: 1 if x == 'Y' else 0)\n",
    "merged_df['Weather - Other'] = merged_df['Weather - Other'].apply(lambda x: 1 if x == 'Y' else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1602,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot_encoded = pd.get_dummies(merged_df['Highest Injury Severity Alleged'], prefix='Highest Injury Severity Alleged')\n",
    "\n",
    "# put 'unknown' col to the end\n",
    "columns = [col for col in one_hot_encoded.columns if 'Unknown' not in col]\n",
    "columns.append([col for col in one_hot_encoded.columns if 'Unknown' in col][0])\n",
    "one_hot_encoded = one_hot_encoded[columns]\n",
    "\n",
    "for col in reversed(one_hot_encoded.columns):\n",
    "    merged_df.insert(merged_df.columns.get_loc('Highest Injury Severity Alleged') + 1, col, one_hot_encoded[col])\n",
    "\n",
    "merged_df.drop(columns= 'Highest Injury Severity Alleged', inplace= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1603,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df['Property Damage?'] = merged_df['Property Damage?'].apply(lambda x: 1 if x == 'Yes' else 0)\n",
    "merged_df['CP Any Air Bags Deployed?'] = merged_df['CP Any Air Bags Deployed?'].apply(lambda x: 1 if x == 'Yes' else 0)\n",
    "merged_df['CP Was Vehicle Towed?'] = merged_df['CP Was Vehicle Towed?'].apply(lambda x: 1 if x == 'Yes' else 0)\n",
    "merged_df['CP Contact Area - Rear Left'] = merged_df['CP Contact Area - Rear Left'].apply(lambda x: 1 if x == 'Y' else 0)\n",
    "merged_df['CP Contact Area - Left'] = merged_df['CP Contact Area - Left'].apply(lambda x: 1 if x == 'Y' else 0)\n",
    "merged_df['CP Contact Area - Front Left'] = merged_df['CP Contact Area - Front Left'].apply(lambda x: 1 if x == 'Y' else 0)\n",
    "merged_df['CP Contact Area - Rear'] = merged_df['CP Contact Area - Rear'].apply(lambda x: 1 if x == 'Y' else 0)\n",
    "merged_df['CP Contact Area - Top'] = merged_df['CP Contact Area - Top'].apply(lambda x: 1 if x == 'Y' else 0)\n",
    "merged_df['CP Contact Area - Front'] = merged_df['CP Contact Area - Front'].apply(lambda x: 1 if x == 'Y' else 0)\n",
    "merged_df['CP Contact Area - Rear Right'] = merged_df['CP Contact Area - Rear Right'].apply(lambda x: 1 if x == 'Y' else 0)\n",
    "merged_df['CP Contact Area - Right'] = merged_df['CP Contact Area - Right'].apply(lambda x: 1 if x == 'Y' else 0)\n",
    "merged_df['CP Contact Area - Front Right'] = merged_df['CP Contact Area - Front Right'].apply(lambda x: 1 if x == 'Y' else 0)\n",
    "merged_df['CP Contact Area - Bottom'] = merged_df['CP Contact Area - Bottom'].apply(lambda x: 1 if x == 'Y' else 0)\n",
    "merged_df['CP Contact Area - Unknown'] = merged_df['CP Contact Area - Unknown'].apply(lambda x: 1 if x == 'Y' else 0)\n",
    "merged_df['SV Any Air Bags Deployed?'] = merged_df['SV Any Air Bags Deployed?'].apply(lambda x: 1 if x == 'Yes' else 0)\n",
    "merged_df['SV Was Vehicle Towed?'] = merged_df['SV Was Vehicle Towed?'].apply(lambda x: 1 if x == 'Yes' else 0)\n",
    "merged_df['SV Were All Passengers Belted?'] = merged_df['SV Were All Passengers Belted?'].apply(lambda x: 1 if x == 'Yes' else 0)\n",
    "merged_df['SV Pre-crash Speed - Unknown'] = merged_df['SV Pre-crash Speed - Unknown'].apply(lambda x: 1 if x == 'Y' else 0)\n",
    "merged_df['SV Contact Area - Rear Left'] = merged_df['SV Contact Area - Rear Left'].apply(lambda x: 1 if x == 'Y' else 0)\n",
    "merged_df['SV Contact Area - Left'] = merged_df['SV Contact Area - Left'].apply(lambda x: 1 if x == 'Y' else 0)\n",
    "merged_df['SV Contact Area - Front Left'] = merged_df['SV Contact Area - Front Left'].apply(lambda x: 1 if x == 'Y' else 0)\n",
    "merged_df['SV Contact Area - Rear'] = merged_df['SV Contact Area - Rear'].apply(lambda x: 1 if x == 'Y' else 0)\n",
    "merged_df['SV Contact Area - Top'] = merged_df['SV Contact Area - Top'].apply(lambda x: 1 if x == 'Y' else 0)\n",
    "merged_df['SV Contact Area - Front'] = merged_df['SV Contact Area - Front'].apply(lambda x: 1 if x == 'Y' else 0)\n",
    "merged_df['SV Contact Area - Rear Right'] = merged_df['SV Contact Area - Rear Right'].apply(lambda x: 1 if x == 'Y' else 0)\n",
    "merged_df['SV Contact Area - Right'] = merged_df['SV Contact Area - Right'].apply(lambda x: 1 if x == 'Y' else 0)\n",
    "merged_df['SV Contact Area - Front Right'] = merged_df['SV Contact Area - Front Right'].apply(lambda x: 1 if x == 'Y' else 0)\n",
    "merged_df['SV Contact Area - Bottom'] = merged_df['SV Contact Area - Bottom'].apply(lambda x: 1 if x == 'Y' else 0)\n",
    "merged_df['SV Contact Area - Unknown'] = merged_df['SV Contact Area - Unknown'].apply(lambda x: 1 if x == 'Y' else 0)\n",
    "merged_df['Data Availability - EDR'] = merged_df['Data Availability - EDR'].apply(lambda x: 1 if x == 'Y' else 0)\n",
    "merged_df['Data Availability - Police Rpt'] = merged_df['Data Availability - Police Rpt'].apply(lambda x: 1 if x == 'Y' else 0)\n",
    "merged_df['Data Availability - Telematics'] = merged_df['Data Availability - Telematics'].apply(lambda x: 1 if x == 'Y' else 0)\n",
    "merged_df['Data Availability - Complaints'] = merged_df['Data Availability - Complaints'].apply(lambda x: 1 if x == 'Y' else 0)\n",
    "merged_df['Data Availability - Video'] = merged_df['Data Availability - Video'].apply(lambda x: 1 if x == 'Y' else 0)\n",
    "merged_df['Data Availability - Other'] = merged_df['Data Availability - Other'].apply(lambda x: 1 if x == 'Y' else 0)\n",
    "\n",
    "merged_df.drop(columns= 'Data Availability - No Data', inplace= True)\n",
    "\n",
    "merged_df['Data Availability - Unknown'] = merged_df['Data Availability - Unknown'].apply(lambda x: 1 if x == 'Y' else 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1604,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No         468\n",
      "Yes        203\n",
      "Unknown     28\n",
      "Name: Law Enforcement Investigating?, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(merged_df['Law Enforcement Investigating?'].value_counts())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1605,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.to_csv(\"NHTSA_ADS_cleaned.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "peter",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
